<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>A Beginner&#39;s Guide to Gradient Descent - Understanding the Core of Machine Learning Optimization | </title>
<meta name="keywords" content="">
<meta name="description" content="In this blog, I will try to explain what exactly is gradient descent and it&rsquo;s importance in optimizing machine learning models. Gradient descent is an algorithm (just like million other algorithms that exists in computer science) that is designed to minimize a cost function.
What is a cost function?

Well, it is essentially the error between predicted and actual outputs. Which means if we want to increase our model&rsquo;s accuracy, we need to somehow reduce this cost function and for that we&rsquo;ve got gradient descent.

What is a Gradient?

A gradient is essentially the derivative of a function, which tells us how the output is affected with little variations in input. This gradient, helps us in adjusting model parameters (like weights and biases) to reduce the cost function.

Why Gradient Descent?

It helps us in minimizing the error in predictions, bringing the model closer to the actual data. It iteratively keep adjusting the model&rsquo;s parameters until minimum cost function state is achieved.
This iterative process ensures the model learns and improves its performance.

What are Weights and Biases?
Weights: Weights (W) are parameters that define influence of an input on the model&rsquo;s output.
Biases: Biases (b) is an additional parameter that shifts the model&rsquo;s prediction.">
<meta name="author" content="Shreyy">
<link rel="canonical" href="/posts/gradient/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css" integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF&#43;13Dyqob6ASlTrTye8=" rel="preload stylesheet" as="style">
<link rel="icon" href="/favicon.ico" sizes="any">
<link rel="icon" href="/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
<link rel="apple-touch-icon" href="/favicon.ico">
<link rel="mask-icon" href="/favicon.ico">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="/posts/gradient/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="/posts/gradient/">
  <meta property="og:site_name" content="Shreyy">
  <meta property="og:title" content="A Beginner&#39;s Guide to Gradient Descent - Understanding the Core of Machine Learning Optimization">
  <meta property="og:description" content="In this blog, I will try to explain what exactly is gradient descent and it’s importance in optimizing machine learning models. Gradient descent is an algorithm (just like million other algorithms that exists in computer science) that is designed to minimize a cost function.
What is a cost function? Well, it is essentially the error between predicted and actual outputs. Which means if we want to increase our model’s accuracy, we need to somehow reduce this cost function and for that we’ve got gradient descent. What is a Gradient? A gradient is essentially the derivative of a function, which tells us how the output is affected with little variations in input. This gradient, helps us in adjusting model parameters (like weights and biases) to reduce the cost function. Why Gradient Descent? It helps us in minimizing the error in predictions, bringing the model closer to the actual data. It iteratively keep adjusting the model’s parameters until minimum cost function state is achieved. This iterative process ensures the model learns and improves its performance. What are Weights and Biases? Weights: Weights (W) are parameters that define influence of an input on the model’s output. Biases: Biases (b) is an additional parameter that shifts the model’s prediction.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-12-22T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-12-22T00:00:00+00:00">
    <meta property="og:image" content="/images/gradient.webp">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="/images/gradient.webp">
<meta name="twitter:title" content="A Beginner&#39;s Guide to Gradient Descent - Understanding the Core of Machine Learning Optimization">
<meta name="twitter:description" content="In this blog, I will try to explain what exactly is gradient descent and it&rsquo;s importance in optimizing machine learning models. Gradient descent is an algorithm (just like million other algorithms that exists in computer science) that is designed to minimize a cost function.
What is a cost function?

Well, it is essentially the error between predicted and actual outputs. Which means if we want to increase our model&rsquo;s accuracy, we need to somehow reduce this cost function and for that we&rsquo;ve got gradient descent.

What is a Gradient?

A gradient is essentially the derivative of a function, which tells us how the output is affected with little variations in input. This gradient, helps us in adjusting model parameters (like weights and biases) to reduce the cost function.

Why Gradient Descent?

It helps us in minimizing the error in predictions, bringing the model closer to the actual data. It iteratively keep adjusting the model&rsquo;s parameters until minimum cost function state is achieved.
This iterative process ensures the model learns and improves its performance.

What are Weights and Biases?
Weights: Weights (W) are parameters that define influence of an input on the model&rsquo;s output.
Biases: Biases (b) is an additional parameter that shifts the model&rsquo;s prediction.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "/posts/"
    }
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "A Beginner's Guide to Gradient Descent - Understanding the Core of Machine Learning Optimization",
      "item": "/posts/gradient/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "A Beginner's Guide to Gradient Descent - Understanding the Core of Machine Learning Optimization",
  "name": "A Beginner\u0027s Guide to Gradient Descent - Understanding the Core of Machine Learning Optimization",
  "description": "In this blog, I will try to explain what exactly is gradient descent and it\u0026rsquo;s importance in optimizing machine learning models. Gradient descent is an algorithm (just like million other algorithms that exists in computer science) that is designed to minimize a cost function.\nWhat is a cost function? Well, it is essentially the error between predicted and actual outputs. Which means if we want to increase our model\u0026rsquo;s accuracy, we need to somehow reduce this cost function and for that we\u0026rsquo;ve got gradient descent. What is a Gradient? A gradient is essentially the derivative of a function, which tells us how the output is affected with little variations in input. This gradient, helps us in adjusting model parameters (like weights and biases) to reduce the cost function. Why Gradient Descent? It helps us in minimizing the error in predictions, bringing the model closer to the actual data. It iteratively keep adjusting the model\u0026rsquo;s parameters until minimum cost function state is achieved. This iterative process ensures the model learns and improves its performance. What are Weights and Biases? Weights: Weights (W) are parameters that define influence of an input on the model\u0026rsquo;s output. Biases: Biases (b) is an additional parameter that shifts the model\u0026rsquo;s prediction.\n",
  "keywords": [
    
  ],
  "articleBody": "In this blog, I will try to explain what exactly is gradient descent and it’s importance in optimizing machine learning models. Gradient descent is an algorithm (just like million other algorithms that exists in computer science) that is designed to minimize a cost function.\nWhat is a cost function? Well, it is essentially the error between predicted and actual outputs. Which means if we want to increase our model’s accuracy, we need to somehow reduce this cost function and for that we’ve got gradient descent. What is a Gradient? A gradient is essentially the derivative of a function, which tells us how the output is affected with little variations in input. This gradient, helps us in adjusting model parameters (like weights and biases) to reduce the cost function. Why Gradient Descent? It helps us in minimizing the error in predictions, bringing the model closer to the actual data. It iteratively keep adjusting the model’s parameters until minimum cost function state is achieved. This iterative process ensures the model learns and improves its performance. What are Weights and Biases? Weights: Weights (W) are parameters that define influence of an input on the model’s output. Biases: Biases (b) is an additional parameter that shifts the model’s prediction.\nFor Example, If we consider a model that is trained to determine house prices based on the area (in Sq. m), then in this case area’s impact on the price of the house is weight and other factors such as location, or the number of bedrooms are biases that can shift the prediction.\nHow does Gradient Descent Work? Firstly, we take a starting point (it can be any point as it’s just an arbitary point for us to evaluate the performance). Then the derivative (or slope) is calculated at this point, after which we can use a tangent line to observe the steepness of the slope. Finding out the slope is an important step as it will inform what updates need to be made to the parameters- i.e. the weights and bias. The starting point of the slope will be steeper, but as new parameters are generated, the steepness will gradually reduce and will slowly near zero as we reach the lowest point on the curve. This lowest point on the curve is known as the point of convergence.\nLike in linear regression, we try to find the bet fit line, the goal of gradient is to minimize the error between the predicted y and the actual y. In order to achieve this goal, it requires two data points- a direction and a learning rate. These factors determine the partial derivative calculations of future iterations, allowing it to gradually arrive at the local or global minimum (i.e. point of convergence).\nSteps: 1. Starting Point: Begin with initial parameter values, often set arbitrarily. 2. Calculate the Slope: Determine the gradient at this point to assess the steepness and direction of the slope. 3. Update Parameters: Use the slope to adjust the parameters, iteratively reducing the steepness until the curve flattens at the minimum. 4. Convergence: Continue until the gradient approaches zero, signifying that the minimum has been reached.\nKey Components Learning Rate (α) Determines the size of step we take after each calculation or the variation in input.\nHigh learning rate takes larger steps which speeds up the process but also increases the risk of overshooting the minimum. Low learning rate ensures precise steps but increases computation time as the number of iterations increases. Cost Function The cost function quantifies the error between the predicted output and the actual output.\nIt guides the model by providing feedback, enabling parameter updates to minimize the error. A loss function refers to the error for a single training example, while the cost function averages this error across the entire dataset. ",
  "wordCount" : "631",
  "inLanguage": "en",
  "image":"/images/gradient.webp","datePublished": "2024-12-22T00:00:00Z",
  "dateModified": "2024-12-22T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Shreyy"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/posts/gradient/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "",
    "logo": {
      "@type": "ImageObject",
      "url": "/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="/posts" title="Blogs">
                    <span>Blogs</span>
                </a>
            </li>
            <li>
                <a href="/projects" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="mailto:shreyyshreyy464@gmail.com" title="Contact">
                    <span>Contact</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="/">Home</a>&nbsp;»&nbsp;<a href="/posts/">Blogs</a></div>
    <h1 class="post-title entry-hint-parent">
      A Beginner&#39;s Guide to Gradient Descent - Understanding the Core of Machine Learning Optimization
    </h1>
    <div class="post-meta"><span title='2024-12-22 00:00:00 +0000 UTC'>December 22, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Shreyy

</div>
  </header> 
<figure class="entry-cover">
        <img loading="eager" src="/images/gradient.webp" alt="Cover Photo">
        
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#what-is-a-cost-function" aria-label="What is a cost function?">What is a cost function?</a></li>
                <li>
                    <a href="#what-is-a-gradient" aria-label="What is a Gradient?">What is a Gradient?</a></li>
                <li>
                    <a href="#why-gradient-descent" aria-label="Why Gradient Descent?">Why Gradient Descent?</a></li>
                <li>
                    <a href="#what-are-weights-and-biases" aria-label="What are Weights and Biases?">What are Weights and Biases?</a></li>
                <li>
                    <a href="#how-does-gradient-descent-work" aria-label="How does Gradient Descent Work?">How does Gradient Descent Work?</a></li>
                <li>
                    <a href="#steps" aria-label="Steps:">Steps:</a></li>
                <li>
                    <a href="#key-components" aria-label="Key Components">Key Components</a>
                </li>
            </ul>
        </div>
    </details>
</div>


  
  <div class="post-content"><p>In this blog, I will try to explain what exactly is <strong>gradient descent</strong> and it&rsquo;s importance in optimizing machine learning models. Gradient descent is an algorithm (just like million other algorithms that exists in computer science) that is designed to minimize a <strong>cost function</strong>.</p>
<h3 id="what-is-a-cost-function">What is a cost function?<a hidden class="anchor" aria-hidden="true" href="#what-is-a-cost-function">#</a></h3>
<ul>
<li>Well, it is essentially the error between predicted and actual outputs. Which means if we want to increase our model&rsquo;s accuracy, we need to somehow reduce this cost function and for that we&rsquo;ve got gradient descent.</li>
</ul>
<h3 id="what-is-a-gradient">What is a Gradient?<a hidden class="anchor" aria-hidden="true" href="#what-is-a-gradient">#</a></h3>
<ul>
<li>A gradient is essentially the derivative of a function, which tells us how the output is affected with little variations in input. This gradient, helps us in adjusting model parameters <em>(like weights and biases)</em> to reduce the cost function.</li>
</ul>
<h3 id="why-gradient-descent">Why Gradient Descent?<a hidden class="anchor" aria-hidden="true" href="#why-gradient-descent">#</a></h3>
<ul>
<li>It helps us in minimizing the error in predictions, bringing the model closer to the actual data. It iteratively keep adjusting the model&rsquo;s parameters until minimum cost function state is achieved.</li>
<li>This iterative process ensures the model learns and improves its performance.</li>
</ul>
<h3 id="what-are-weights-and-biases">What are Weights and Biases?<a hidden class="anchor" aria-hidden="true" href="#what-are-weights-and-biases">#</a></h3>
<p><strong>Weights:</strong> Weights (W) are parameters that define influence of an input on the model&rsquo;s output.
<strong>Biases:</strong> Biases (b) is an additional parameter that shifts the model&rsquo;s prediction.</p>
<p>For Example, If we consider a model that is trained to determine house prices based on the area (in Sq. m), then in this case area&rsquo;s impact on the price of the house is weight and other factors such as location, or the number of bedrooms are biases that can shift the prediction.</p>
<h3 id="how-does-gradient-descent-work">How does Gradient Descent Work?<a hidden class="anchor" aria-hidden="true" href="#how-does-gradient-descent-work">#</a></h3>
<p>Firstly, we take a starting point (it can be any point as it&rsquo;s just an arbitary point for us to evaluate the performance). Then the derivative (or slope) is calculated at this point, after which we can use a tangent line to observe the steepness of the slope. Finding out the slope is an important step as it will inform what updates need to be made to the parameters- i.e. the weights and bias. The starting point of the slope will be steeper, but as new parameters are generated, the steepness will gradually reduce and will slowly near zero as we reach the lowest point on the curve. This lowest point on the curve is known as the point of convergence.</p>
<p>Like in linear regression, we try to find the bet fit line, the goal of gradient is to minimize the error between the predicted y and the actual y. In order to achieve this goal, it requires two data points- a direction and a learning rate. These factors determine the partial derivative calculations of future iterations, allowing it to gradually arrive at the local or global minimum (i.e. point of convergence).</p>
<h3 id="steps">Steps:<a hidden class="anchor" aria-hidden="true" href="#steps">#</a></h3>
<p><strong>1. Starting Point:</strong> Begin with initial parameter values, often set arbitrarily.
<strong>2. Calculate the Slope:</strong> Determine the gradient at this point to assess the steepness and direction of the slope.
<strong>3. Update Parameters:</strong> Use the slope to adjust the parameters, iteratively reducing the steepness until the curve flattens at the minimum.
<strong>4. Convergence:</strong> Continue until the gradient approaches zero, signifying that the minimum has been reached.</p>
<h3 id="key-components">Key Components<a hidden class="anchor" aria-hidden="true" href="#key-components">#</a></h3>
<ol>
<li>
<p><strong>Learning Rate (α)</strong>
Determines the size of step we take after each calculation or the variation in input.</p>
<ul>
<li><em>High learning rate</em> takes larger steps which speeds up the process but also increases the risk of overshooting the minimum.</li>
<li><em>Low learning rate</em> ensures precise steps but increases computation time as the number of iterations increases.</li>
</ul>
</li>
<li>
<p><strong>Cost Function</strong>
The cost function quantifies the error between the predicted output and the actual output.</p>
<ul>
<li>It guides the model by providing feedback, enabling parameter updates to minimize the error.</li>
<li>A <em>loss function</em> refers to the error for a single training example, while the <em>cost function</em> averages this error across the entire dataset.</li>
</ul>
</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share A Beginner&#39;s Guide to Gradient Descent - Understanding the Core of Machine Learning Optimization on x"
            href="https://x.com/intent/tweet/?text=A%20Beginner%27s%20Guide%20to%20Gradient%20Descent%20-%20Understanding%20the%20Core%20of%20Machine%20Learning%20Optimization&amp;url=%2fposts%2fgradient%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share A Beginner&#39;s Guide to Gradient Descent - Understanding the Core of Machine Learning Optimization on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fposts%2fgradient%2f&amp;title=A%20Beginner%27s%20Guide%20to%20Gradient%20Descent%20-%20Understanding%20the%20Core%20of%20Machine%20Learning%20Optimization&amp;summary=A%20Beginner%27s%20Guide%20to%20Gradient%20Descent%20-%20Understanding%20the%20Core%20of%20Machine%20Learning%20Optimization&amp;source=%2fposts%2fgradient%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share A Beginner&#39;s Guide to Gradient Descent - Understanding the Core of Machine Learning Optimization on reddit"
            href="https://reddit.com/submit?url=%2fposts%2fgradient%2f&title=A%20Beginner%27s%20Guide%20to%20Gradient%20Descent%20-%20Understanding%20the%20Core%20of%20Machine%20Learning%20Optimization">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share A Beginner&#39;s Guide to Gradient Descent - Understanding the Core of Machine Learning Optimization on whatsapp"
            href="https://api.whatsapp.com/send?text=A%20Beginner%27s%20Guide%20to%20Gradient%20Descent%20-%20Understanding%20the%20Core%20of%20Machine%20Learning%20Optimization%20-%20%2fposts%2fgradient%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share A Beginner&#39;s Guide to Gradient Descent - Understanding the Core of Machine Learning Optimization on telegram"
            href="https://telegram.me/share/url?text=A%20Beginner%27s%20Guide%20to%20Gradient%20Descent%20-%20Understanding%20the%20Core%20of%20Machine%20Learning%20Optimization&amp;url=%2fposts%2fgradient%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="/"></a></span> · 

    <span>
        Made by <a href="https://x.com/shreyyzsh">Shreyy</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
